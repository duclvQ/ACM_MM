(PERSON5) [PERSON10] is online?
(PERSON9) [PERSON10] is online, yeah.
(PERSON5) OK.
So I take this chair.
(PERSON9) Yeah.
We cannot, we cannot meet in the meeting room -
(PERSON5) Yeah.
(PERSON9) Because, um, because there are -
(PERSON5) Yeah.
What is it there?
(PERSON9) <unintelligible/>.
I don't know what is it.
(PERSON5) OK.
Um, so you <unintelligible/>.
(PERSON9) <unintelligible/> people.
No, I only like excused that I'm interrupting and -
(PERSON5) OK.
(PERSON9) <laugh/>
And this is it.
(PERSON5) Like it should be <unintelligible/> I don't know when I shouldn't go back and ask like what's that cause we like should use the room.
(PERSON9) Yeah.
(PERSON5) Like it's, it's not people from our floor I think so...
(PERSON9) Yeah, it's not -
(PERSON5) Yeah.
(PERSON9) <unintelligible/> so -
(PERSON5) Hm.
I don't know.
(PERSON9) They are not discussing something connect to this building.
(PERSON5) Hm.
I don't know.
(PERSON10) Maybe it's the physics slowly overtaking, um, our -
<laugh/>
Communal space.
(PERSON5) Yeah, I don't know.
So what do you think?
Should I just go there and ask or should I leave it for now?
(PERSON10) Hm, I would leave it.
(PERSON5) OK.
(PERSON9) Yeah, let's leave it for now and we, we -
(PERSON5) Hm.
(PERSON9) We need to solve it somehowe because I, I, I also heard, um, [PERSON12] on Friday that he would like to do the testing on, in that, in that room so -
(PERSON5) He has a testing room.
<laugh/>
(PERSON9) He has a testing room but he, he, he, um, he said that there is a not-, not air conditioing in his room -
(PERSON5) Hm.
(PERSON9) So he would like to use that air conditioning because -
(PERSON5) Ah, OK.
Yeah, yeah.
(PERSON9) So that, that's sure then cannont -
I don't know.
So -
(PERSON5) OK.
Like temporarily, yeah.
That I can just <unintelligible/>.
(PERSON9) Temporarily, yeah.
I, I think it's fine but we should -
(PERSON5) Hm, hm.
(PERSON9) We should have, um, schedule somewhere.
(PERSON5) Yeah, yeah.
Well, we do.
But they ignore it.
<laugh/>
(PERSON9) They ignore it, yeah.
But, yeah.
Because I heard, I heard that he, he ask [PERSON15] -
(PERSON5) Hm.
(PERSON9) But then he dissapeared so I cannot -
(PERSON5) Hm, hm, hm.
(PERSON9) Tell him anything about it.
(PERSON5) Hm, hm, hm.
(PERSON9) And that, that also we, we would like to use that room.
<laugh/>
(PERSON5) Hm, hm, hm.
(PERSON9) So...
(PERSON5) OK.
(PERSON9) So we should, we should -
(PERSON5) We should have a talk a bit.
(PERSON9) Yeah.
(PERSON5) Hm.
(PERSON9) OK.
Um, so the first point of this meeting.
Um, we, we will need reviewers and we should choose -
Um, [PERSON2] told me that we should, ideally we should choose three reviewers and -
(PERSON5) Hm.
(PERSON9) We, we should, um, we should let them know and we should have the confirmation them that, that they are, they would review our book.
(PERSON5) Yeah.
(PERSON9) And then, then [PERSON2] would choose two of them and oficially ask them to review our book.
So -
(PERSON5) So what kind of people should it be?
Like -
(PERSON9) Outside -
(PERSON5) Ph.d., doctors or?
(PERSON9) It wa-
Um, he didn't specified.
Only, um, hm, hm, hm...
<another_language/>
(PERSON5) <another_language/>
(PERSON9) <another_language/>
(PERSON5) <laugh/>
(PERSON9) <another_language/>
(PERSON5) Hm.
(PERSON9) So do you have anyone, [PERSON5]?
(PERSON5) Hm.
<laugh/>
<censored/>.
<laugh/>
(PERSON9) Yeah, I think some-, something, something in between that.
<laugh/>
Yeah.
Cause for example I, I could ask [PERSON1].
(PERSON5) Hm.
(PERSON9) But I, I think he is too -
(PERSON5) Hm.
(PERSON9) Too, um, how to say it.
<another_language/>
(PERSON5) Hm.
<laugh/>
Yeah.
So we could like ask people like, like anes give us some of his students maybe or some, some <unintelligible/>
(PERSON9) I, I don't know what it be, what it be, we -
Yeah.
I can, I can ask <unintelligible/> -
(PERSON5) Hm.
(PERSON9) Because I think he is -
(PERSON5) Hm.
(PERSON9) Yeah.
We could do that.
(PERSON5) Hm.
(PERSON9) But I don't, I work for him.
(PERSON5) Yeah.
(PERSON9) For, for -
(PERSON5) Hm.
(PERSON9) Um, <unintelligible/>.
<laugh/>
(PERSON5) So I think -
Yeah, I think the [ORGANIZATION1] not be so greats so to ask like some super cool professors cause it will be waiting there from my point so, um -
(PERSON9) Yeah, I can, I can ask, um, [PERSON2], wh-
(PERSON5) Hm.
(PERSON9) What better they should to be like -
(PERSON5) Yeah.
(PERSON9) Docents or -
(PERSON5) Yeah, like -
(PERSON9) Associates.
(PERSON5) I don't know.
So I expect probably they should be at least doctors...
So let's try that like some, some young post docs could be good reviewers I guess.
(PERSON18) Yeah.
<censored/>.
(PERSON9) OK.
And do you, do you have anybody else that they, other research group or?
You know?
(PERSON5) To we can look at the paper <unintelligible/> and -
(PERSON9) Yeah.
(PERSON5) Just ask <unintelligible/> still be team and -
<laugh/>
(PERSON9) Or we cited most if -
(PERSON5) Hm.
(PERSON9) We could ask him because we would like -
(PERSON5) Hm.
(PERSON9) We'll be happy that we <unintelligible/> -
(PERSON5) <unintelligible/> [PERSON14].
<laugh/>
(PERSON9) Yeah, we can ask [PERSON14], who, who is he from?
Um...
(PERSON5) I don't know.
<other_noise/>
(PERSON18) [ORGANIZATION4].
(PERSON9) What?
(PERSON5) What?
(PERSON10) [ORGANIZATION4].
(PERSON9) [ORGANIZATION4].
<laugh/>
(PERSON5) Hm.
<laugh/>
(PERSON9) But we don't, we don't know him so we, we don't know what, what he would say -
(PERSON5) Hm.
(PERSON9) If he -
(PERSON5) Hm.
(PERSON9) So I don't know if, if he will be officially assigned by, by [PERSON2] and, and, um, he will say that, that book is like -
<laugh/>
(PERSON5) Hm.
(PERSON9) <censored/>.
<laugh/>
I don't know what it be, what to do then.
What do we -
We can choose another one or -
<laugh/>
<censored/>.
<laugh/>
Um -
(PERSON5) Oh, come on and let's do that.
<laugh/>
(PERSON9) <unintelligible/> it is but maybe -
<laugh/>
<unintelligible/> goes through reviewers -
(PERSON5) Hm.
(PERSON9) It's <unintelligible/> bad.
(PERSON5) Hm.
(PERSON18) And on the other hand it, it shouldn't be anyone what who might work closer right now I guess.
(PERSON5) Well, that's we don't know either.
Like how, how grave the conflict of interest is.
(PERSON9) I don't know.
(PERSON2) only wrote that, um...
They should be independent.
(PERSON18) Hm.
(PERSON9) But that means that they should be not from [ORGANIZATION7] and possibly not from [LOCATION2].
<another_language/>
(PERSON5) <another_language/>
(PERSON10) <another_language/>
(PERSON9) OK, so I, I can ask him.
(PERSON5) Yeah.
Like of course it's easy to ask people that we know -
<laugh/>
Because we know them so -
<laugh/>
But maybe it's a conflict of interest.
<another_language/>
Conflict of interest.
(PERSON9) <another_language> Aha. </another_language>
(PERSON5) So [PERSON2] says to not what a regerest about conflict of interest.
(PERSON18) Hm.
(PERSON5) So probably it can be people who we know.
<laugh/>
I don't know if it at least like shouldn't be people with whom we alter papers maybe.
But since we're call <unintelligible/> in dependences -
<laugh/>
We are call throughs with half of the NLP people.
I don't know like about some past doctoral students of [ORGANIZATION7] where you that OK or not.
(PERSON18) Hm.
(PERSON5) Like I don't know [PERSON11] or -
(PERSON9) <unintelligible/> [PERSON11].
<laugh/>
Who are the past students of [ORGANIZATION7] who -
(PERSON5) [PERSON13].
<laugh/>
(PERSON9) [PERSON13].
(PERSON16) <unintelligible/>.
(PERSON5) And then he says they shouldn't be from [LOCATION2] so does it mean like we should originally not be from [LOCATION2] -
<laugh/>
Or now they shouldn't be in [LOCATION2] or never.
<laugh/>
(PERSON9) Hm.
(PERSON5) Yeah, I think that you <unintelligible/> with more details or -
<laugh/>
I think we want to go for it or like lowest possible -
<laugh/>
(PERSON10) Yeah, someone from [LOCATION1].
(PERSON5) Yeah.
<laugh/>
<unintelligible/> that's a great guy.
I don't know how much he speaks English actually but -
<laugh/>
<unintelligible/>.
Or the, these, these Bulgarian's guys, um, [PERSON7] and -
(PERSON9) [PERSON7] and <unintelligible/>.
(PERSON5) Yeah.
<laugh/>
Like -
<laugh/>
<censored/>.
(PERSON9) <another_language/>
Where do they would understand our book?
<censored/>.
<laugh/>
I don't know.
Like if we think the book will be good maybe should get good reviewers cause they will give us good feedback.
If we will think the book won-, won't be good anyway so -
<laugh/>
<censored/>.
(PERSON9) So we can -
Maybe we can ask if we have the book ready we can ask someone to review the book and send him the book and ask, ask him to do you think this book is OK -
(PERSON5) Hm.
(PERSON9) And if you think would you, would you be the reviewer?
<laugh/>
(PERSON5) Like we <unintelligible/> is so first if you think it's OK -
<laugh/>
Only if you think through that you should be reviewer.
<laugh/>
I think it's not independent.
<laugh/>
(PERSON9) Yeah, that's right.
That is -
<laugh/>
(PERSON5) <unintelligible/> I know -
(PERSON9) Like what, what you mean by -
Yeah.
(PERSON5) Yeah.
I know tuns of people so I can ask a lot of people but -
<laugh/>
I just need to know like some rules like more, more details.
So where you at should be at least a doctor, whether it should be someone with knowling to [LOCATION2] or I don't know.
(PERSON9) OK.
So I, I will ask, I will ask [PERSON2] what, what -
(PERSON5) Hm.
(PERSON9) Where the...
<another_language/>
(PERSON5) Hm.
Yeah.
I don't know what's <unintelligible/> in our situations anyway so -
(PERSON9) OK, so it should be less, less strict then for, for, um, for the reviewers of disertation so but you can have check reviewers of disertation.
(PERSON5) Hm.
(PERSON9) Like -
(PERSON5) Hm.
(PERSON9) Non [ORGANIZATION7], from outside -
<another_language/>
(PERSON5) Hm.
(PERSON9) Maybe not in, maybe only outside...
The our faculty.
(PERSON5) Hm.
(PERSON9) So I don't know.
(PERSON5) OK, so -
I guess, yeah.
(PERSON9) Yeah, I can -
(PERSON5) You should try to ask [PERSON2] again and...
(PERSON9) I can, I can ask [PERSON4], he's, he's my friend for years.
(PERSON5) Hm.
(PERSON9) He's best Bohemian.
(PERSON5) Hm.
(PERSON9) He'll definitelly say, OK, this book is great.
<laugh/>
And he is, he is associate professor so -
(PERSON5) Hm.
<laugh/>
Yeah, we could ask, um, what's his name, um...
<another_language/>
[PERSON3].
(PERSON9) [PERSON3]?
(PERSON5) <another_language/>
(PERSON9) Hm.
<laugh/>
(PERSON5) <another_language/>
(PERSON9) OK, so, um, what we have in the agenda?
Nothing else.
<laugh/>
So let's go to -
<laugh/>
What is new in the book.
(PERSON5) Yeah.
If anyone push anything in the morning?
(PERSON18) Yes, I did.
(PERSON5) OK, so I shell pull and make again.
Did you fix the missing figure?
(PERSON18) Yes, I did.
(PERSON5) Great.
(PERSON9) I think I have the version from which is one hour like old so it, um, should be updated.
(PERSON5) OK.
(PERSON9) Um...
I can share the screen.
Where is it?
(PERSON18) I fixed it immediately after you, you wrote it, or probably not.
Anyway I, I fix it immediately after I turn on the, the computer in the morning.
<laugh/>
(PERSON5) OK.
(PERSON9) OK.
(PERSON18) Which was like 7:30 or something like that.
(PERSON9) OK, so I, probably I have the current version.
OK.
(PERSON9) Today, today I, today I went through the all book, um, I, I haven't read it all but just skim, skimed it like parts and I, I read the pre-, preface which is very, very good.
Does [PERSON18], [PERSON18]-
(PERSON5) Yes.
(PERSON9) You, you wrote it's [PERSON18].
(PERSON18) Yes, I did.
(PERSON9) Yeah.
Yeah.
I think this is, this is great.
Um...
<laugh/>
I, I skimm the, the first section.
OK.
So then we have the notable models.
So that, that, that is, that was written by [PERSON18] and [PERSON5] or, or only by [PERSON18]?
(PERSON18) Hm.
<censored/>.
<laugh/>
From [PERSON5]'s text but other words is through the know by me, I guess.
(PERSON9) So it's Word-, Word2vec, um, RNN's, transformer.
<other_noise/>
Transformer is very short.
(PERSON5) Hm.
(PERSON18) Yes, because everything is explained.
(PERSON9) Everything is ready in, um, yeah.
(PERSON18) The, the only thing I say, you take the transformer encoder and transformer decoder which I described in section 1.
(PERSON9) Yeah.
(PERSON18) 1.3.2 and, and that's it.
(PERSON9) Yeah.
Kove, [PROJECT2] and [PROJECT1] I suppose.
Great.
And GPT is missing so that's what, um, I suppose GPT should be there, here as well.
(PERSON18) Um.
(PERSON9) So do you think?
Or it shou-, it can be -
(PERSON18) <unintelligible/> that, you don't have the latest version by the way there're like four more paragraphs about [PROJECT1].
(PERSON9) Sorry?
(PERSON18) Um, that you don't have the latest version but it doesn't matter.
(PERSON9) You -
I don't have the latest version?
Aha.
(PERSON18) Yeah.
(PERSON5) Yeah, the image is still missing so you don't have the latest version.
(PERSON9) Aha.
OK.
<laugh/>
Yeah, so probably my morning was earlier than [PERSON18]'s morning.
<laugh/>
(PERSON5) Yeah, so you wrote already something about GPT?
(PERSON18) No.
But I almost finished writing about [PROJECT1].
(PERSON5) Yeah, OK.
(PERSON18) And the <unintelligible/> or -
(PERSON9) So should I -
OK.
Um, yeah, I read this interpretation and I, I think that, um, there should be something like introductory, introductory paragraph to this.
Um, because when I, when I, when I read it I, it post like, um, bigger step for me.
Maybe it should be -
So this, this was written by [PERSON10], yeah?
(PERSON10) Um, yes.
Written, well.
(PERSON9) Maybe some introductory, um, OK that neural networks are like black boxes we don't know.
There are, there are plenty of parametres and we don't know nothing about it and we would like to know what, how about their behaviour and, um, um, or is, it is here?
Yeah, it is, it is in the to do, um, so <unintelligible/> black boxes, we need to interpret them.
<other_noise/>
<laugh/>
(PERSON10) Yeah.
(PERSON9) Because there is -
Yeah, there are, there are so far there are many to does, so there is not, not much text.
So I don't know how, how much text you are planning to write there.
(PERSON10) Well, I don't know either.
I'm like -
I think it would be at least 3 pages.
Maybe more if I have time to read, um, the paper are in to do section.
(PERSON9) Yeah. Maybe I, I, I, I would restructure it in the way that first I would write, OK, so these so all, all the methods, al-, all the, all the architectures we described in the previous chapter are very complex and, um, vital like black box and we need to interpret them and then you can -
<other_noise/>
Um, you can ask, what is, what is really interpretability may-
But, but anyway you -
Yeah, these tell like to, um, transparency or post hoc interpretation and you will use the -
We will, we will, in this book we will use the post hoc, post hoc interpretability so -
Um...
(PERSON10) OK, I will do that.
(PERSON9) Yeah.
Or if, if anyone of you have another, um, idea to do it?
(PERSON18) No, no, no.
This is fine.
(PERSON9) Yeah and then -
Yeah and what is, what is probing, um...
So maybe we should, we should, um, write here about, um, what are the, um, the methods we will use for, for interpret-, um, interpretations because, um, OK, the probing originally it was for all the post hoc interpr-, interpretation methods.
It was like, it was the synonymum of the post hoc interpretation.
Um, but then, um, this is mainly meant as, it's mainly meant as, um, classif-, as a classifiers.
So maybe we should somehow distinguish the methods we will, we will use in our book.
Um, so what about for example I don't know sup-, supervised and unsupervised?
Or how to would -
If it is not probing what is it?
(PERSON10) Hm.
(PERSON18) Well, would you, um, for in senses hear it and [PERSON6] paper when they do the projection, um, up the stace su-, such that there's which are close an independency tree should be closed in a vector space.
Do you call this probing or no?
(PERSON9) I, I would, I would call it probing because they, they're like training the, the metrices.
So they are training the parametres.
Um...
Is it all, this is only linear projection but it is something...
I don't know.
(PERSON18) Yeah, but it's not -
It, it's still a classifier.
(PERSON9) It's not a classifier but yeah.
(PERSON5) They call it probing, yeah.
(PERSON9) They, yeah, they call it probing but yeah, you are right that is not classifier but there is, something is trained there.
So, yeah, it's not classifier because it's, it's training for, for, yeah, for the best distances that are, that, that match the, the dependency trees.
So, yeah, this is something in the middle, yeah.
<laugh/>
But, but OK.
If, if he, if he would, um, if he divides the metod to supervised and unsupervised so this probing would be supervised because it is a training on some, um, gold data.
(PERSON18) Yes, um.
(PERSON9) <unintelligible/>, um, the PTA and, um, what [PERSON10] is doing on word embeddings or, um, like the attentions, um, gettig, getting the dependancy tree from attention metrices or this is rather unsupervised or because we are not training anything.
(PERSON5) Hm.
Yeah, I would probably say that we understood probing as supervised.
(PERSON18) Um, what about the thing that you upgold birth it and, and the best paper from the last year nackled it.
Um, they, <unintelligible/>, um, just mask some words in sentences and looked, looked, if, if bird is capable of, of filling the gaps in tactical correctly and the paper from nakl was similar for multimodel translation there were masking words and, and wor-, and observing if the model can recover the missing words from, um, from the image.
Is that probing?
(PERSON5) No.
(PERSON9) I wouldn't say it probing.
<laugh/>
(PERSON18) Alright.
(PERSON9) <unintelligible/>.
(PERSON18) Both, both of the papers call it probing by the way.
(PERSON5) Hm.
(PERSON9) Aha.
OK.
So, so do you think it, it should be, um, like the, the different methods like, like general method should be here in this chapter or?
(PERSON18) Probably, yes.
(PERSON9) Because, because later, like the attentions are, it is like, um, mainly not probing where else the, there's [PERSON5]'s section is mainly probing so, um, it can be described in that chapters as well.
And [PERSON10] in, in like the how it is called.
Um, neural language model embedding, so embeddings...
Um, here in the methods are -
There is not much probing here I suppose.
(PERSON10) Hm.
Well, not yet but there should be.
(PERSON9) OK.
So, so maybe, yeah.
Maybe the, like the methods in general should be described, described here after the, um, after the, like the introduction to interpretation.
(PERSON5) Yeah, I think it, it should be descripted here in the section, yeah.
(PERSON9) Yeah.
Because -
Yeah.
What, what is -
(PERSON5) Hm.
(PERSON9) What follows is the hidden states grape on insto input words, which is, which is -
So this is, this is copied from, from your, um, this is still text copied from your, um -
(PERSON5) Probably...
(PERSON18) Yeah, yeah, yeah.
This is from [PERSON5]'s chapter.
(PERSON9) This is from [PERSON5]-
(PERSON18) It's -
First into the -
(PERSON9) Yeah, this is, this is, yeah.
(PERSON18) Models and then I, then I moved it here.
(PERSON9) Yeah, this is the text that -
Yeah, that is very, or not very similar but it's, it's described as same as -
(PERSON5) Hm.
(PERSON9) I described in my part and so it should be probably here because it is common for all the, all the fa-, at least for our two, our two attention and contextual -
(PERSON5) Hm.
(PERSON9) And representate parts.
(PERSON5) Yeah, yeah.
For word embeddings it doesn't have this problem.
There is only the problem if you have subwords.
I don't know if we talk about that somewhere.
(PERSON9) Um...
(PERSON5) Like that -
Yeah, we'll talk -
We need very to representations but we usually have subword representations.
(PERSON9) Yeah.
I don't know.
Me, yeah.
I, I will go through my text -
(PERSON5) Hm.
(PERSON9) Maybe it is there so I -
(PERSON5) Hm.
(PERSON9) I can copy part of it to here and then we, we'll somehow to mentioned.
<other_noise/>
(PERSON5) Yeah, so we -
So I don't know.
Do we agree on, on what probing is?
Or we should probably not like define it, we should rather probably discuss that there is like in general idea of probing in the sense of just looking for something and then some people understand it in a more specific way of training -
(PERSON18) Hm.
(PERSON5) And classifire.
(PERSON9) Yeah.
(PERSON5) I think we shouldn't like define it here we should rather review how people use it truly and what they can mean by that.
(PERSON9) Yeah.
(PERSON18) Yeah, that, that we still use the word -
(PERSON9) If we, if we will use it in our text then -
(PERSON18) Yeah.
(PERSON9) We should, we could at least define not like, not maybe definition but we should say what we mean by the def-, by the probing and...
(PERSON5) Hm.
<other_noise/>
Do we have -
What, what do we call it if it's not probing?
<other_noise/>
Is analysis?
(PERSON10) Not probing.
<laugh/>
(PERSON5) And there is lot of stuff which is not probing.
<laugh/>
Like going shopping.
(PERSON9) Also some of the probing is not probing.
<laugh/>
So we can interpret a neural networks by going shopping.
(PERSON5) Hm.
<laugh/>
Yeah.
We could talk about unsupervise probing when it's not trained.
I don't know if anybody uses such a term.
(PERSON9) Yeah.
I don't know will unsupervised analysis or -
Yeah, it is analysis.
Probing is analysis as well.
(PERSON5) Cause -
Yeah.
(PERSON9) Like unsupervised probing.
(PERSON5) Yeah, yeah.
(PERSON9) So...
(PERSON5) Yeah.
So some people call it probing when it is -
(PERSON8) Featchures analysis.
(PERSON9) Sorry?
(PERSON8) Featchures analysis.
(PERSON9) Yes.
(PERSON5) Hm.
(PERSON9) Yeah, but, yeah, but the, but important is that is, is unsupervised.
(PERSON8) Hm.
(PERSON9) Because if you, if you probing for parter speech text it's also featchures analysis maybe.
(PERSON5) Hm.
Yeah.
And you can probe for parter speech text like by really training the classifier.
(PERSON8) Hm.
(PERSON5) Or you can just do some clustering or you can do key nearest neibourghs.
I think that somewhere that they do key nearest neibourghs, well, maybe one nearest neibourgh even, so it's not trained but it's a classification.
(PERSON9) OK.
I have some notes here but I cannot read them.
<laugh/>
(PERSON5) OK, it's secret.
<laugh/>
(PERSON9) Yeah, this -
We already discussed that.
OK.
(PERSON5) Hm.
(PERSON9) Um...
OK, so who is, who will write this, um, like that, this distinctions?
Or this definitions?
Or this, this, um, um, types of methods?
(PERSON10) Hm...
Well, I can start with something and then you could all read it and then we can disagree and see where it goes.
(PERSON9) Yeah.
If, if you, [PERSON10] if you -
Yeah, you have many, many no-, like interesting papers here so, so you plan to include them?
(PERSON10) Yes.
(PERSON9) OK.
<laugh/>
And when?
<laugh/>
(PERSON10) As soon as possible.
<laugh/>
(PERSON9) So, OK.
So yeah, also -
Or maybe I can, I can write a bit about, about the, about the -
(PERSON10) OK.
(PERSON9) About the recurrent methods and y-, you can write about, you can, about, um, like the, um, from these interesting papers if there is any interesting things to add.
(PERSON10) At least the first two I think should be added there.
The rest it's not so important.
(PERSON9) OK, so if we -
OK.
So and then these, it should be also here the hidden states coresspond and input words because it's common to, to out to that the following sections.
So, um, so I will, I will, I will move there some text of mine and, um, you'll, you will match it somehow.
Yeah.
And I, anyway I will need to rewrite my, um, my introductory sections about it because there is, there is a lot of common <unintelligible/> and -
(PERSON5) Hm, hm.
(PERSON9) Um...
(PERSON5) Yeah.
(PERSON9) OK.
(PERSON5) I think like, um, yeah, I'm not sure actually.
I understood that like first we should have all the content somewhere and then we can merge it and move it and -
(PERSON9) Yeah, of course.
(PERSON5) Yeah, yeah, yeah.
(PERSON9) Of course.
(PERSON5) So -
(PERSON9) Yeah, first -
Yeah, if you don't have all, all your content -
(PERSON5) Hm, hm.
(PERSON9) Of first writed content.
(PERSON5) Hm, hm.
(PERSON9) So that we have, yeah, only information at least somewhere.
(PERSON5) Yeah.
(PERSON9) At least on time.
(PERSON5) Yeah.
(PERSON9) And yeah, and see.
So because we have the deadline for, um, for all the content -
(PERSON5) End of June?
(PERSON9) Um, the next, it's next week, yeah?
(PERSON10) Hm.
(PERSON9) Yeah.
Next Tuesday.
So next Tuesday all the content should be there.
(PERSON10) OK.
<other_noise/>
(PERSON9) So stories from the neural networks.
So [PERSON10], what do you, um, what are your plans with your, um, with your chapter?
(PERSON10) Sorry, what?
What are my plans or?
(PERSON9) Yeah.
(PERSON10) Well, my plan is to write and write and then it will be written.
I don't know, right now -
(PERSON9) This is starting, this is starting like language model before neural networks and neural language models and I don't know where it this should -
Maybe this should go to the notable models.
Um, doesn't it?
(PERSON10) Well, maybe.
Um, yeah, that I'm...
I, I think that I will like merge this with notable models in the end.
(PERSON9) Because if we had a section about mod-, notable models I, I would, I would start, um -
(PERSON10) Yeah.
(PERSON9) Vector -
(PERSON10) I think that -
(PERSON9) <unintelligible/> things and these are rather like description of the, of the models.
Like 4, 4.1 and 4.2.
(PERSON10) Yeah and some, some of what is, um, in 4.1 and 4.2 already is in the notable models so I will have to delete it and, yeah, just say that you should look in to the previous section.
(PERSON9) You don't need to delete it, you ju-, um, maybe there is more information here so just -
(PERSON10) Yeah, um, merge it.
Yeah.
(PERSON18) Yeah.
I, I avoided speaking about any interpretation, I just technically decribed how the models look like and how they are used in NLP tasks.
(PERSON9) So I, I think -
(PERSON18) So all -
(PERSON9) Um, starting by thre-, 4.3, um, like the...
(PERSON18) 4.3.
(PERSON9) We side like the skip gram and the, and the aritmetic, um, vector aritmetic and the, so this, this, this, this can be, this is an interpretation of, of -
(PERSON18) Yeah, yeah.
(PERSON9) So it may, it may start by this, um, like the word analogies on, on word embeddings.
(PERSON10) Hm.
(PERSON9) I would say.
That, that's my opinion.
(PERSON10) Yeah.
(PERSON9) If you have another, please say it.
Um, OK, so this is, these, these are word analogies.
Then, um...
<other_noise/>
Um, so Glove...
I'm sorry I, I haven't read this part today so I don't know what is put in.
(PERSON10) Well, this is, um, unfinished text about Glove.
(PERSON9) But it, it is that Glove is -
So it's only des-, description of Glove or it is, um...
(PERSON10) No, als-
It's mostly interpretation.
(PERSON9) OK.
OK.
So then the visualisation.
Which is <unintelligible/> and TS, TSN, nice -
OK, then component analysis and...
So you plan to include, include your, your results of component an-, of PCA?
(PERSON10) Yes, PCA definitelly.
Maybe ICA, I don't know.
(PERSON9) So it, it's still lot of, lof of text here is missing.
(PERSON10) Yes.
Also lot of images are missing.
(PERSON9) So, so basely please add it there.
At least, at least copy, copy the text from the papers so that we know how much and what we have there.
<other_noise/>
So that we, we have all the, all the content there, um...
OK.
So these are the, um, unsupervised [PROJECT4].
So that's...
(PERSON10) Hm.
Hm, hm, hm, hm.
(PERSON9) What is it?
Or I, I know what is it but what, what's that in the, in the paragraph?
(PERSON10) Well, this is just a basic description and I'm actually not sure if this is going to stay there because I don't really know if there are really interesting papers like interpreting unsupervised [PROJECT4].
Hm.
(PERSON9) OK, so this is probably be, be deleted.
(PERSON10) Yeah.
(PERSON9) And sentiment it's, yeah, it's from your, from your papers.
And, um, the GPT?
(PERSON10) Yeah, again I'm not sure.
Maybe I will just you know delete this section because I think that there are more, it's more interesting to look into attentions in the transformer architecture in general than to interpret word embeddings which even aren't there.
I would just you know the simplifying, I would just concentrate on word embeddings in this chapter and leave this generative models.
(PERSON9) OK.
OK, so please in the next week do the....
(PERSON10) Yeah.
(PERSON9) All the deletions and all the deletions and, um, move the -
(PERSON10) Deletions mostly.
(PERSON9) And add it then and mostly additions.
Yeah and all the, all the figures, um, add.
Add them there.
No matter format.
So, so that all, they, so that we know what it, what is it going to be there.
And so we can see what, what -
Yeah, we can see the contents, um.
(PERSON18) By the way is, is anyone going to talk specifically about the GPT models?
Because if not then there is a question if, if they should be -
(PERSON9) Yeah.
(PERSON18) In, in the notable models.
(PERSON5) I mention, um, some GPT stuff but I don-, I don't yet know if there will be anything <unintelligible/> like, yeah, some of the contextual embeddings, probings, also probe GPT.
(PERSON18) Hm.
(PERSON5) And say something about it.
I just don't yet know whether GPT is special in some way so that we need to go in to detail on it.
(PERSON9) Yeah.
(PERSON5) But I very probably will mention some results that are on GPT.
(PERSON9) Yeah, I, I will also, I have also have one, one paper Monday analyse the, um, analyse the attention, um, attentions, attention way of GPT's and they -
Yeah, so I will mention it as well so -
But -
(PERSON18) Alright.
(PERSON9) Yeah, so I would let it there so far and we, we will see but yeah.
(PERSON18) Oh, leaving it there means writing it.
<laugh/>
(PERSON9) Because it is not, that is written.
(PERSON18) No.
(PERSON9) Aha.
So maybe...
(PERSON18) That I, I, I can write two or three paragraphs just, just briefly say that, that is a language model that, that it's a transfor-, transformer without an encoder.
(PERSON9) Yeah.
(PERSON5) Yeah.
I think, yeah.
That's, that's probably reasonably, like nothing too detailed, um -
(PERSON9) Yeah, not to spend to much time on it -
(PERSON5) Yeah.
(PERSON9) It's like -
(PERSON5) Yeah, yeah.
(PERSON9) Two paragraphs about GPT and then we will see whether how much we -
(PERSON5) Hm.
Yeah.
Yeah, that sounds good.
(PERSON9) OK.
And then there is the, in debiassing.
So [PERSON10] you, do you plan to write about this debiassing thing?
(PERSON10) Yeah.
(PERSON9) So, so far you, it is like one paragraph, like four, four lines.
(PERSON10) Yeah, right now it's just one paragraph.
It will be longer.
There is a figure somewhere.
(PERSON9) Yeah, the figure is -
Yeah, here is a figure.
(PERSON10) Yeah.
So I want to write enough to like explain this figure at least.
<other_noise/>
(PERSON9) OK.
Dog is barking.
<laugh/>
(PERSON5) Who's dog is it?
<laugh/>
(PERSON10) Also, um, at the end of the chapter I would like to show you some images which I was planning to use but now I'm not really sure if -
<other_noise/>
If I can, if I can.
So there is a link in the discussion on the left.
(PERSON9) Where?
Discussion on the left?
(PERSON5) What discussion?
(PERSON10) Um, events.
I don't know.
It's on the left.
(PERSON5) <unintelligible/> -
(PERSON10) Under the [PROJECT3] [ORGANIZATION2].
You can click the message thing and there's a link.
(PERSON9) Hm.
(PERSON10) Or I can share my screen.
(PERSON9) OK.
So let's share your screen.
(PERSON10) Hm.
<other_noise/>
<laugh/>
(PERSON9) <another_language/>
(PERSON18) It's a green rectangle.
On a white background.
(PERSON5) Hm.
(PERSON10) Wait the minute.
Wait a minute.
Um, um, um, OK.
Now.
Now do you see yourself on my screen?
(PERSON9) Yeah.
(PERSON10) Yes.
OK.
Well -
(PERSON9) I'm at least three times there.
<laugh/>
(PERSON10) Here on the left you can click events and you get the link here.
(PERSON5) Hm.
(PERSON10) But I can also just click the link and show you.
Right.
(PERSON9) OK.
(PERSON10) But there's, there's this image.
That's from Glove web and there are these bands.
These are embeddings.
Like every column is an embedding and, and the most frequent words are on the right.
And it shows these bands and they claim that other embeddings have similar structure.
So I tried that and I did Word2vec on the [ORGANIZATION3] and I took FastTex the, that was trained on Czech [ORGANIZATION6] -
(PERSON5) Hm.
(PERSON10) And Word2vec looks like this.
(PERSON5) So there is nothing?
(PERSON10) There is no bands it's just noise.
FastTex looks like this.
<laugh/>
There is a -
(PERSON9) There is one band.
(PERSON10) One band, maybe more bands.
But then -
(PERSON5) Hm.
(PERSON10) There's much, much less noise than in the original image.
And now I'm trying to train Glove on the [ORGANIZATION3] and it was running overnight and then it failed.
(PERSON5) Hm.
(PERSON10) So now I'm reconfigured it and I'm trying again.
So, yeah, I don't know what to make of it.
(PERSON5) Hm.
(PERSON10) Like -
(PERSON5) Hm, well...
We could just include the, the images and show that the embeddings different but I don't know if we know why or?
Like -
I, I would, would for including it but yeah, I'm not sure what to say about it.
(PERSON18) Hm.
Yeah.
(PERSON10) Tdddd.
Um.
Ttttm.
Hm, hm, hm.
(PERSON5) Yeah.
(PERSON10) The Glove website says that the horizont to band result from the fact that mul-, multiplicatly interactions in the model occure component wise.
(PERSON5) Hm.
(PERSON10) And -
(PERSON18) <laugh/>
Have you idea what that means?
(PERSON5) Yeah.
<laugh/>
(PERSON10) They say that -
(PERSON9) We write the book for [PERSON17].
(PERSON5) Yeah.
(PERSON10) They say that this featchure is not unique to Glove.
In fact I'm annowere of any model for word vectors learning that avoids this issue.
(PERSON5) So we'll see once Glove finishes for you if, if you get it at least for Glove.
(PERSON10) Yeah.
Then right the -
I'm, I'm not sure why are they calling it an issue but anyway they're saying that all the models should have this property and I'm not really seeing it in Word2vec and the FastTex this strange.
(PERSON18) And it's trained on English.
That to, the two plots you showed it's English, isn-, isn't it?
(PERSON10) Um, no.
Like OK.
(PERSON18) It's Czech.
(PERSON10) That's other scheme.
OK, the, the original plot is on English, this is on Czech.
(PERSON18) Alright.
(PERSON10) So, but like that shouldn't really matter, should it?
(PERSON18) I don't know.
(PERSON5) So we should try it on English as well, I guess.
If it's not much work.
(PERSON10) Yeah, I'd like to do that and that's, that's, the other question is what data should I use for English.
(PERSON5) [ORGANIZATION6]?
(PERSON10) Hm.
Well, yeah, that was boring, but OK.
I don't know anything better actually.
(PERSON5) Hm.
We have one, one [ORGANIZATION6].
(PERSON10) Hm.
(PERSON18) Yeah, what reason that we don't see the bands in czech could be that similar, um, that forms of the same lema got similar embeddings but the forms are of different frequences.
(PERSON10) Hm, well, but still if you look at the original image -
Which one is it?
Hm.
<other_noise/>
Here.
You can see that some of the bands spent almost the whole vocabulary.
(PERSON18) Yeah.
(PERSON10) And I'm only because I'm lazy and computers are lazy as well I'm only doing it for the 10000 most frequent words, so the bands should be even more pronounced here then in this in, in -
(PERSON5) Hm.
(PERSON10) In this -
(PERSON18) Hm.
(PERSON10) This image.
So I'm only creating like the right most part of this.
(PERSON18) Hm.
(PERSON10) So the band should be clearly visible.
(PERSON18) Yeah.
You can try down-, downloading some, some already trained embeddings for English and, and have a look how it looks like.
It should be easy.
(PERSON10) OK, yeah.
(PERSON5) Just now you can download FasText embeddings trained for any language.
(PERSON18) Yeah.
(PERSON10) Yeah, that -
These are actually this strange image is from FastText embeddings that I downloaded, um, and that were trained on [ORGANIZATION6] -
(PERSON5) Hm.
(PERSON10) And this is for Czech.
(PERSON5) So you didn't train this?
(PERSON10) No, I didn't train this one.
I trained to Word2vec and now I'm training hopefully, I'm training Glove although -
It looks the same before.
It takes lot of time and then it suddenly says, I don't know, zero lines processed and -
Well, we will see.
Also I got an email that I'm approaching, um, the quota on, on -
What is this?
Elnet splash MS.
(PERSON5) Yeah, so you can move it else where.
(PERSON10) Well, bu-, I'm also approaching my quota on Elnet splash spack.
<laugh/>
(PERSON18) Hm.
(PERSON10) So I'm like need to go through my files, find something that's not usefull anymore and delete it.
(PERSON5) Yeah.
(PERSON10) I don't know -
(PERSON5) You can move that to Elnet dep.
I think the depo is maybe without quota.
(PERSON10) Yeah.
I don't know how -
(PERSON5) <unintelligible/> things for experiments right now but you don't want to delete it yet.
(PERSON10) I don't know how but I somehow manage to collect like half of terrabyte of data and...
(PERSON18) Alright.
(PERSON5) Oh, yeah, you should clean it up then.
<laugh/>
(PERSON10) On Elnet list spack and I have a lot of, yeah.
So that's what I'm doing right now.
(PERSON9) OK.
So...
Yeah.
Yeah, for my, for my chapter -
Yeah.
I will need to rewrite, rewrite the introductions and yeah.
Yeah, I, I still, I will lot of, lot of, um, contents to write so...
Um...
So let's, let's keep it maybe, maybe [PERSON5], um, can talk.
(PERSON5) Yeah, so -
(PERSON9) The last what, what he has -
(PERSON5) Um...
<other_noise/>
(PERSON9) Well, I will share my screen.
(PERSON5) I will not, yeah.
(PERSON9) Share my screen, share my screen.
Yeah, and..
(PERSON5) Yeah, you have it there, yeah.
Yeah.
So I, um, didn't really change the, the beggining of it.
Um...
Yeah, I'm rearranging, um, the stop, um, here a bit.
Basically I'm now working on integrating stuff from other papers.
What I work on now is, yeah, OK, I added some things here.
Yeah, I, I don't know how to structure thing like, yeah, grammatical error correction.
So, um, yeah, like they, they did probing for the task of grammatical error correction but I structured it by, not by the probing task but by the abstractions that you looked for.
So I'm not 100 % sure like OK what structures do you look for when you look for grammatical error correction.
So at the moment I have it under semantics.
It's probably rather a syntax but, hm...
<laugh/>
(PERSON18) Did do they show any examples of the errors?
(PERSON5) Yeah, they do 17 tasks in this paper so I think they don't say much about the task but probably I will have to see what they do, like what they refer to -
(PERSON18) OK.
(PERSON5) To understand <unintelligible/> what it is.
Anyway, yeah, what I worked on now is, um, this thing?
Yeah.
Um, yeah, this, this section.
So how is the, um, information distrubuted across layers of RNN and transformers.
And so this is kind of, um, how it say, model section how I would like to have it so, so to say how the things work and then just add the citations to the, the papers that say it.
(PERSON18) Hm.
(PERSON9) How, how many layers did be great in the RNN only?
(PERSON5) 2.
(PERSON9) Only 2.
(PERSON5) Usually 2.
(PERSON9) <unintelligible/>, yeah.
(PERSON5) Some probings they have more.
So Kove and [PROJECT2] are defined as having 2 layers.
(PERSON9) Yeah.
(PERSON5) And then usually when they compare it to [PROJECT1] and some probing papers then they try it let's take 4 layer ELM-, [PROJECT2], 8 layer [PROJECT2] and let's see what happens.
(PERSON9) Yeah.
So, so for this maybe, maybe of the first layer captures all these linguistic abstractions and the second layer is for like just task specific one.
(PERSON5) Yeah, yeah, yeah, yeah.
(PERSON9) Like you, um, I think I was some second layer so where you know from, from many -
(PERSON5) Hm.
(PERSON9) Layers are there because -
(PERSON5) Yeah.
(PERSON9) That -
Yeah, I know fr-, that from [PROJECT1].
(PERSON5) Hm.
(PERSON9) Like first, first half of the layers are like -
(PERSON5) Hm.
Yeah, yeah.
(PERSON9) Um -
(PERSON8) The second is RNN.
(PERSON9) Yeah, these are RNN but yeah, but -
(PERSON5) Hm, yeah.
(PERSON9) It doesn't matter.
(PERSON5) Yeah.
So yeah, I should explain that more I guess.
<laugh/>
(PERSON9) Yeah, it seems that there aren't many -
(PERSON5) Yeah, yeah, yeah.
(PERSON9) There aren't many layers.
(PERSON5) Yeah, yeah.
Usually not.
Yeah.
<laugh/>
Yeah and then for, for transformers I even tried to like, um, make to bullet list of, of the findings of how information is distributed in transformers.
<other_noise/>
I know obviously like all the findings come from I don't know like there are 5 different papers that basically say the same thing so I just tried to choose one for each claim.
I, I'm aware you just to repeat all the papers that say it or -
(PERSON9) Oh, yeah.
So this is all, this, all this things are like [PERSON14] do just embeddings.
<unintelligible/>, aha.
(PERSON5) Yeah, yeah.
(PERSON9) <unintelligible/>.
(PERSON5) Only you.
I know.
(PERSON9) Yeah.
<other_noise/>
(PERSON5) So like initialization morfological abstractions there are like 10 papers that find that -
(PERSON9) Yeah. [PERSON5] That, that morfology -
(PERSON9) It's maybe -
Yeah.
(PERSON5) Comes in the first layers.
(PERSON9) This, this maybe somehow aligned to the, to the, um, like attention, attention <unintelligible/>.
(PERSON5) Hm.
(PERSON9) Cause there are also like the initial layers are like, um, <unintelligible/> looking at the next.
(PERSON5) Hm.
(PERSON9) And this is like the morfological things.
(PERSON5) Hm, hm.
(PERSON9) Then the, the, um, the -
(PERSON5) Hm.
(PERSON9) In the, in the half of the, of the layer, um, like the, in, layer in the middle -
(PERSON5) Hm.
(PERSON9) There are mainly capturing syntactic thins and -
(PERSON5) Hm.
(PERSON9) Then there are some coreferential links as -
(PERSON5) Hm.
(PERSON9) Um, like, like coreferen-, um, um, the layers of the attention heads which capturing the coreferent-
(PERSON5) Hm. [PERSON9] Coreferences.
(PERSON5) Hm, hm.
(PERSON9) <unintelligible/>.
(PERSON5) Yeah.
(PERSON9) So it maybe somehow <unintelligible/>.
(PERSON5) Hm, hm.
(PERSON9) Yeah.
(PERSON5) Yeah.
Um.
(PERSON9) It's open.
<another_language/>
<unintelligible/>.
(PERSON5) Um, yeah.
So, um, yeah, what's somebody said is that, um, yeah, they, they talk about like the locality of, of the representation so basically that yeah.
In the, in the first layers you guess things they are more lokal like morfology, then constituency syntax and then you get like long ran-, more long range stuff like dependency syntax and then you can get through some semantics, coreference where you kind of need the whole sentence.
So I guess that similar to the attentions.
That, um, like theoretically even in the first layer you can look anywhere in the sentence but in practice it seems that it's more like convolution like so that the deeper layers have a wider spen.
So, so this is probably something you already say in the attention, um, attention chapter.
(PERSON9) Hm. [PERSON5] And I can just maybe refer to that.
(PERSON9) Yeah.
(PERSON5) And we know that, yeah.
Um, it gets less and less local so, so, um, it's, it's kind of related to that.
That's nice.
(PERSON8) <unintelligible/> about this distribution information that I <unintelligible/> like thing -
(PERSON5) Hm.
(PERSON8) Visualizations, um, for actually most different transformers.
(PERSON5) Aha.
(PERSON8) Um, but, and it's only -
(PERSON5) Hm. [PERSON8] For syntactic information.
(PERSON5) Hm, hm.
(PERSON8) It say <unintelligible/>.
(PERSON5) Hm, but that's nice.
That's nice.
So yeah.
Um, it would be very nice to include.
Yeah, I think it's exactly this picture here.
<laugh/>
(PERSON9) So this is, this is, this is your figure?
(PERSON8) Yeah, it's my figure.
(PERSON5) Hm, hm.
That's very nice.
Hm.
So the, the darker it's more -
(PERSON8) Yeah, the darker is better.
(PERSON5) Yeah, yeah, yeah.
(PERSON8) So this is like it's always normalize.
(PERSON5) Hm.
Hm.
(PERSON8) Says like the best car is -
(PERSON5) Hm, hm.
(PERSON8) Because they do sources probing -
(PERSON5) Hm.
(PERSON8) So the <unintelligible/> are better and the those are, um, like, um, yeah.
(PERSON5) Hm, hm, hm.
(PERSON8) So just extracting tree -
(PERSON5) Hm, hm.
(PERSON8) From random attentions.
(PERSON5) Yeah, yeah, yeah, yeah.
(PERSON8) And, um, sorry I cannot show you yet.
(PERSON5) Hm.
You can just -
<laugh/>
(PERSON9) You can, you can -
<laugh/>
Um, you can show them like -
(PERSON5) On the <unintelligible/>.
<laugh/>
(PERSON8) You have -
If you have the paper in the -
Yeah.
<laugh/>
It is -
I don't know whether it's like really visible.
(PERSON18) Yes.
<laugh/>
It's -
(PERSON9) I think it's -
They can, they can see it.
(PERSON8) <unintelligible/> show.
<laugh/>
(PERSON9) It is -
It's, yeah, very nice figure, colorful.
(PERSON5) Yeah.
It's like old style way of sharing screen.
<laugh/>
(PERSON9) OK, so don't, don't hesitate to include your, your -
(PERSON5) Yeah, yeah.
(PERSON9) Your nice figure into our book.
(PERSON5) Yeah.
So that we, we know that it's -
You have the -
(PERSON5) Hm.
(PERSON9) Yeah.
You have the rights to -
(PERSON8) Yeah, yeah.
Just I need to find the, the best place because it can be like for probing -
(PERSON9) Just -
(PERSON8) And that can't be for -
(PERSON9) Put it, put it somewhere -
(PERSON5) OK.
(PERSON9) So that we know that we have, we have this figure.
(PERSON5) Yeah, yeah, yeah.
(PERSON9) And so -
(PERSON8) OK.
(PERSON5) Yeah, I think -
Yeah, yeah, so this, this is really -
Yeah, this should be show in abstraction across layers so I think it fit here nicely.
(PERSON8) OK, OK.
(PERSON5) Yeah and, and fortunately that's really the section that I, I was working on now so it's, it's in a good shape already rather -
(PERSON9) Yeah.
(PERSON5) So I tried to -
(PERSON9) Maybe, maybe I could, I could do as, um, similar, I could do the similar figure for this, like different types of, um, of attention heads across layers as well.
(PERSON5) Hm, hm.
(PERSON9) So like distribution.
Not the abstraction, but distribution of like, yeah...
(PERSON5) Hm.
(PERSON9) Um, like different patterns -
(PERSON5) Hm, hm, hm.
(PERSON9) Of, of, um, attention heads.
(PERSON5) Hm.
Hm.
(PERSON9) And it would be, it would be similar because at the end, um -
(PERSON5) Hm.
(PERSON9) The first layers are looking at the, an adjustement, um, position and at the end there are looking at like, like -
(PERSON5) Hm, hm.
(PERSON9) OK.
(PERSON5) Yeah.
I tried to do something like in the sense like the, the teny paper they have, um, some kind of nice but weird figures, um, where they tried to show which is, what, what's where, so, so their probing does some attentions over the layers.
So when they probe for morfology so in, into which layers it looks.
So when they probe for name entities into which layers it looks.
Um, so I definitely cannot recreate the, um, image.
I could kind of copy paste it from the paper, I don't know.
Um, so but, but I tried to interpret in, in this table so, um, they take the 20 <unintelligible/> birds and this is kind of an estimate from their groves like on which layers do these abstractions and merge and then, um...
Yeah, the last column is rewrite just interpreting the, the image so it's the desolution rather peaks or is it kind of flat that is everywhere across -
(PERSON9) Hm. [PERSON5] Model layers so...
It's kind of stupid without the image I guess.
<laugh/>
But it will be better if we have the image for, um, for, at least for syntax I guess.
<censored/>.
(PERSON18) Hm, hm.
<censored/>.
But they said, yeah, it will be ugly, which it probably will I guess.
<laugh/>
(PERSON18) Yeah.
(PERSON5) Um, I -
Well, I don't know if we can some whole extract it in a, in a good quality from the pdf.
I, I don't -
(PERSON18) Hm.
(PERSON5) I can just like copy, copy it with, with the pdf viewer tools, um...
(PERSON9) No, you, you can, um, you can simply, um, open it or import it into Inkscape.
(PERSON5) Aha, OK.
(PERSON9) And then degroup -
(PERSON5) Hm.
(PERSON9) Ungroup.
<laugh/>
Ungroup it.
(PERSON5) Hm.
(PERSON9) And use on that image.
So it is possible.
(PERSON18) Yeah, you can, you can open it in Inkscape and, um, if you, if you willish to extract the paper from, um, do the image from the paper, you can save it, um, as a, as a pdf with text, um, in, in a latech format.
Um, which means that all, all the text labels, um, will be saved in a separate, separate tix file and, and you will include the tix file and what will happend is, is that, that you have a pdf without the text.
And, and the text will be rendered during compilation so it will be the same font as, as we have in the book.
(PERSON5) Yeah.
If, um, the image, the original image in the paper is, is not relikely on image but, but the produced somehow within latex.
(PERSON18) Oh, or even if you export, I don't know, a pdf from Matplotlib or, or, or, or G-
(PERSON5) <unintelligible/> the image in the paper so I don't know whether the numbers and text are text and numbers <unintelligible/> or just an image.
I didn't try so I have to see what happend when I open it in Inkscape.
OK.
I can try and we can, we can always delete it.
<laugh/>
(PERSON18) Yeah.
(PERSON5) I just -
<laugh/>
Um, yeah, doesn't know what, what, what's a good <unintelligible/>.
(PERSON18) There, there are <unintelligible/> if, if you can select, um, the, the, if you can select the text in, at pdf with your, with your mouse then you can export it.
(PERSON5) OK, I'll see.
<other_noise/>
Um, I take just a note.
OK.
(PERSON9) OK.
So, um...
(PERSON5) Um...
(PERSON9) Anything else?
(PERSON5) I think, yeah, that's, that's kind of what I did.
Yeah, um, yeah.
So yeah, so, so yeah.
What I tried to do here, yeah.
So we want to know in which layers what is and in the paper they have like three different ways of, of saying this and always it gets different results because it's hard to really say it so I just tried to interpret it somehow but -
(PERSON18) Hm.
(PERSON5) That's probably what we kind of want so we want to interpret the findings of other people, I don't know.
(PERSON9) Yeah.
(PERSON5) Yeah, yeah, yeah.
I guess, yeah, I didn't do much more, yeah.
(PERSON9) OK, so -
(PERSON18) Can -
Can you rename the very last section of your chapter so that is not conclusion so we don't have like several conclusions after each other?
(PERSON5) Um...
I don't know.
Um, I removed some of the other conclusion so I think we now have just two.
<laugh/>
Now we have to put in a summary so we don't actually have two conclusions anymore.
(PERSON18) Oh.
Alright.
(PERSON9) That was also this conclusion of your chapter?
(PERSON5) Yeah, yeah, yeah.
(PERSON9) Yeah.
(PERSON5) Yeah.
(PERSON9) Yeah.
(PERSON5) So, yeah.
So I, I actually -
Yeah, this, this is what I tried to do so -
(PERSON18) Alright.
(PERSON5) If you now look at the -
Where is the contents?
Yeah.
So my section and there is the conclusion and there is the summary.
So it is -
(PERSON18) Alright.
(PERSON5) Kind of good to me I guess but...
(PERSON18) Hm.
(PERSON9) Yeah, I think it's good.
(PERSON18) Well, it's then a question is if, um, if you have a conclusion if, if all the other chapters sure have a conclusion as well.
(PERSON5) Well, I don't know.
I don't know if I will -
(PERSON9) I, I can have a conclu-
(PERSON5) Yeah.
(PERSON9) I can -
(PERSON5) But [PERSON10] deffinitely have a summary at the end of his chapter.
(PERSON9) OK.
I have nothing but I can, yeah, I can have a conclusion.
(PERSON10) Well, I -
(PERSON9) Yeah.
(PERSON5) I don't know what I do have there.
(PERSON9) I, I also conclude something.
(PERSON5) Yeah.
<laugh/>
Like I think it's not a bad idea to, to just summ up like the main findings.
(PERSON9) Yeah, yeah.
(PERSON18) Yeah, yeah.
(PERSON9) OK.
<other_noise/>
OK, great.
So I, I suggest, um, I will, um, maybe I will create a table.
Who is, who is going -
What is, what is missing so the -
What is missing in the, in the book and who is going to write it.
(PERSON5) Hm.
Yeah, like for me it's really thi, this to do sections so -
<laugh/>
(PERSON9) These, these are through the -
<laugh/>
Maybe you, you can, you can add to the table -
(PERSON5) Hm. [PERSON9] Um, everything we will, we would -
(PERSON5) Hm.
(PERSON9) Have or should -
<laugh/>
(PERSON5) Hm.
(PERSON9) You don't have.
And, um, if you are, um, if you are write-, if you, where you write it you will just <unintelligible/> that it, it's finished. [PERSON5] Yeah.
So, so -
(PERSON9) So that we know -
(PERSON5) Yeah.
(PERSON9) That it progress because we have last week -
(PERSON5) Yeah, yeah.
(PERSON9) To, to write it.
(PERSON5) Yeah.
(PERSON9) Um, yeah.
(PERSON5) So, so my to do.
Yeah.
Yeah, I want include all the papers in my to do section but like I have to look into them and see if they add something which I still don't have there and is imporant and if so then I will add them.
And then like, like this, this merging of the information so that I, so that is not like just a review of the papers but it's extract information -
(PERSON9) Yeah.
(PERSON5) And, and present it nicely so...
(PERSON9) Yeah, but first -
(PERSON5) Yeah.
Yeah.
(PERSON9) We, we, we should have all the content -
(PERSON5) Yeah, yeah, yeah.
(PERSON9) Be on.
So I, I will, I will go through, through the book on-, once more and I will, I will wri-, write that, um...
I, I will create the table, um, of missing things and who should do it and yeah.
So that we, we know.
OK.
So anything else?
(PERSON8) So -
(PERSON9) Yeah.
Try to, try to think about the reviewers.
(PERSON5) Hm.
(PERSON18) Hm.
(PERSON9) I will, I will ask [PERSON2] and OK.
So see you next week.
(PERSON5) Yeah.
Meeting is the at what time, I don't know.
(PERSON9) Yeah, meeting, um, we will it scheduled.
Yeah, stop <unintelligible/>.
This was -
(PERSON5) What?
(PERSON9) OK.
Yeah, so 11?
(PERSON18) And what about the meetings in July?
There is this on, on 13th of July then we have -
(PERSON9) Yeah, 13th of July because the week before I'm not here and it's, it's the [ORGANIZATION5] conference as well I think.
(PERSON8) It's on <unintelligible/>.
(PERSON18) No, it's the week before.
(PERSON5) 6th to 10th.
(PERSON9) 6th to 10th it's [ORGANIZATION5], [ORGANIZATION5].
(PERSON18) Yes.
(PERSON9) And I'm not, I'm not here and I'm, I will be outside the any internet connection and, um, yeah, so -
(PERSON18) Good for you.
<laugh/>
(PERSON9) So we will -
<laugh/>
Yeah.
So we will, we will see the next week what have and we can, yeah.
(PERSON18) Alright.
(PERSON9) Because it, it's, it's Tuesday and then, yeah, then it's Monday, the 13th it's Monday.
So it, it's in -
(PERSON18) Yep.
(PERSON9) It is in the 13 days.
Yeah, it, it's easy to -
<laugh/>
Compute it.
OK.
And the next meeting would be, I don't know whether.
How you are here in, in July but, um...
I, I think we will, we hav-, we will , we will see.
OK.
So, so but, OK.
The next meeting is T-, um, the next week and then we will, we will see.
OK?
(PERSON18) Alright.
(PERSON10) Hm.
(PERSON18) Have a good day, bye.
(PERSON5) Thanks.
Bye, bye.
(PERSON9) Bye, bye.
(PERSON8) You too.
(PERSON10) Bye.
<laugh/>
(PERSON9) <another_language/>
(PERSON5) <another_language/>
(PERSON10) Bye.
